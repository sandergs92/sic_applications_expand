{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:50:44 cloud:58] Found existing object /home/sander/.cache/torch/NeMo/NeMo_2.0.0/stt_en_fastconformer_hybrid_large_streaming_multi/a22a1091ef0b90f40b4e99859c44b15e/stt_en_fastconformer_hybrid_large_streaming_multi.nemo.\n",
      "[NeMo I 2024-12-06 15:50:44 cloud:64] Re-using file from: /home/sander/.cache/torch/NeMo/NeMo_2.0.0/stt_en_fastconformer_hybrid_large_streaming_multi/a22a1091ef0b90f40b4e99859c44b15e/stt_en_fastconformer_hybrid_large_streaming_multi.nemo\n",
      "[NeMo I 2024-12-06 15:50:44 common:826] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2024-12-06 15:50:45 mixins:173] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 15:50:46 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - - /raid/local//bucket1/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket2/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket3/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket4/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket5/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket6/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket7/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket8/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 25\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket1/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket2/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket3/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket4/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket5/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket6/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket7/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket8/audio__OP_0..8191_CL_.tar\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size:\n",
      "    - 72\n",
      "    - 64\n",
      "    - 56\n",
      "    - 48\n",
      "    - 40\n",
      "    - 32\n",
      "    - 24\n",
      "    - 16\n",
      "    \n",
      "[NeMo W 2024-12-06 15:50:46 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2024-12-06 15:50:46 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:50:46 features:305] PADDING: 0\n",
      "[NeMo I 2024-12-06 15:50:46 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2024-12-06 15:50:46 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 15:50:46 rnnt_loop_labels_computer:270] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: No `cuda-python` module. Please do `pip install cuda-python>=12.3`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:50:46 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 15:50:46 rnnt_loop_labels_computer:270] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: No `cuda-python` module. Please do `pip install cuda-python>=12.3`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:50:47 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/sander/.cache/torch/NeMo/NeMo_2.0.0/stt_en_fastconformer_hybrid_large_streaming_multi/a22a1091ef0b90f40b4e99859c44b15e/stt_en_fastconformer_hybrid_large_streaming_multi.nemo.\n",
      "[NeMo I 2024-12-06 15:50:47 hybrid_rnnt_ctc_bpe_models:431] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2024-12-06 15:50:47 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 15:50:47 rnnt_loop_labels_computer:270] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: No `cuda-python` module. Please do `pip install cuda-python>=12.3`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:50:47 hybrid_rnnt_ctc_bpe_models:469] Changed decoding strategy of the RNNT decoder to \n",
      "    model_type: rnnt\n",
      "    strategy: greedy_batch\n",
      "    compute_hypothesis_token_set: false\n",
      "    preserve_alignments: null\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    fused_batch_size: null\n",
      "    compute_timestamps: null\n",
      "    compute_langs: false\n",
      "    word_seperator: ' '\n",
      "    rnnt_timestamp_type: all\n",
      "    greedy:\n",
      "      max_symbols_per_step: 10\n",
      "      preserve_alignments: false\n",
      "      preserve_frame_confidence: false\n",
      "      tdt_include_duration_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "      loop_labels: true\n",
      "      use_cuda_graph_decoder: true\n",
      "      max_symbols: 10\n",
      "    beam:\n",
      "      beam_size: 5\n",
      "      search_type: default\n",
      "      score_norm: true\n",
      "      return_best_hypothesis: false\n",
      "      tsd_max_sym_exp_per_step: 50\n",
      "      alsd_max_target_len: 2.0\n",
      "      nsc_max_timesteps_expansion: 1\n",
      "      nsc_prefix_alpha: 1\n",
      "      maes_num_steps: 2\n",
      "      maes_prefix_alpha: 1\n",
      "      maes_expansion_gamma: 2.3\n",
      "      maes_expansion_beta: 2\n",
      "      language_model: null\n",
      "      softmax_temperature: 1.0\n",
      "      preserve_alignments: false\n",
      "      ngram_lm_model: null\n",
      "      ngram_lm_alpha: 0.0\n",
      "      hat_subtract_ilm: false\n",
      "      hat_ilm_weight: 0.0\n",
      "      tsd_max_sym_exp: 50\n",
      "    temperature: 1.0\n",
      "    durations: []\n",
      "    big_blank_durations: []\n",
      "    \n",
      "[NeMo I 2024-12-06 15:50:47 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2024-12-06 15:50:47 rnnt_decoding:730] Joint fused batch size <= 0; Will temporarily disable fused batch step in the Joint.\n",
      "[NeMo I 2024-12-06 15:50:47 hybrid_rnnt_ctc_bpe_models:469] Changed decoding strategy of the RNNT decoder to \n",
      "    model_type: rnnt\n",
      "    strategy: greedy\n",
      "    compute_hypothesis_token_set: false\n",
      "    preserve_alignments: false\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    fused_batch_size: -1\n",
      "    compute_timestamps: null\n",
      "    compute_langs: false\n",
      "    word_seperator: ' '\n",
      "    rnnt_timestamp_type: all\n",
      "    greedy:\n",
      "      max_symbols_per_step: 10\n",
      "      preserve_alignments: false\n",
      "      preserve_frame_confidence: false\n",
      "      tdt_include_duration_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "      loop_labels: true\n",
      "      use_cuda_graph_decoder: true\n",
      "      max_symbols: 10\n",
      "    beam:\n",
      "      beam_size: 5\n",
      "      search_type: default\n",
      "      score_norm: true\n",
      "      return_best_hypothesis: false\n",
      "      tsd_max_sym_exp_per_step: 50\n",
      "      alsd_max_target_len: 2.0\n",
      "      nsc_max_timesteps_expansion: 1\n",
      "      nsc_prefix_alpha: 1\n",
      "      maes_num_steps: 2\n",
      "      maes_prefix_alpha: 1\n",
      "      maes_expansion_gamma: 2.3\n",
      "      maes_expansion_beta: 2\n",
      "      language_model: null\n",
      "      softmax_temperature: 1.0\n",
      "      preserve_alignments: false\n",
      "      ngram_lm_model: null\n",
      "      ngram_lm_alpha: 0.0\n",
      "      hat_subtract_ilm: false\n",
      "      hat_ilm_weight: 0.0\n",
      "      tsd_max_sym_exp: 50\n",
      "    temperature: 1.0\n",
      "    durations: []\n",
      "    big_blank_durations: []\n",
      "    \n",
      "[NeMo I 2024-12-06 15:50:47 features:305] PADDING: 0\n",
      "2559\n",
      "47\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 6.56 ---\n",
      "Chunk 1: \n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.17 ---\n",
      "Chunk 2: \n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.87 ---\n",
      "Chunk 3: \n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.80 ---\n",
      "Chunk 4: \n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.02 ---\n",
      "Chunk 5: well\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.03 ---\n",
      "Chunk 6: well\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.68 ---\n",
      "Chunk 7: well i don't\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.30 ---\n",
      "Chunk 8: well i don't\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.74 ---\n",
      "Chunk 9: well i don't wish\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.93 ---\n",
      "Chunk 10: well i don't wish to\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.03 ---\n",
      "Chunk 11: well i don't wish to see\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.08 ---\n",
      "Chunk 12: well i don't wish to see\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.80 ---\n",
      "Chunk 13: well i don't wish to see it any\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.09 ---\n",
      "Chunk 14: well i don't wish to see it any more\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.98 ---\n",
      "Chunk 15: well i don't wish to see it any more\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.76 ---\n",
      "Chunk 16: well i don't wish to see it any more observ\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.06 ---\n",
      "Chunk 17: well i don't wish to see it any more observed\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.21 ---\n",
      "Chunk 18: well i don't wish to see it any more observed\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.79 ---\n",
      "Chunk 19: well i don't wish to see it any more observed ph\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.02 ---\n",
      "Chunk 20: well i don't wish to see it any more observed phoe\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.59 ---\n",
      "Chunk 21: well i don't wish to see it any more observed phoebe\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.87 ---\n",
      "Chunk 22: well i don't wish to see it any more observed phoebe\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.82 ---\n",
      "Chunk 23: well i don't wish to see it any more observed phoebe turn\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.61 ---\n",
      "Chunk 24: well i don't wish to see it any more observed phoebe turning\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.83 ---\n",
      "Chunk 25: well i don't wish to see it any more observed phoebe turning away\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.79 ---\n",
      "Chunk 26: well i don't wish to see it any more observed phoebe turning away\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.73 ---\n",
      "Chunk 27: well i don't wish to see it any more observed phoebe turning away her\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.55 ---\n",
      "Chunk 28: well i don't wish to see it any more observed phoebe turning away her eyes\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.86 ---\n",
      "Chunk 29: well i don't wish to see it any more observed phoebe turning away her eyes\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.99 ---\n",
      "Chunk 30: well i don't wish to see it any more observed phoebe turning away her eyes\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.48 ---\n",
      "Chunk 31: well i don't wish to see it any more observed phoebe turning away her eyes\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.09 ---\n",
      "Chunk 32: well i don't wish to see it any more observed phoebe turning away her eyes\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.99 ---\n",
      "Chunk 33: well i don't wish to see it any more observed phoebe turning away her eyes it\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.62 ---\n",
      "Chunk 34: well i don't wish to see it any more observed phoebe turning away her eyes it\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.98 ---\n",
      "Chunk 35: well i don't wish to see it any more observed phoebe turning away her eyes it is\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.15 ---\n",
      "Chunk 36: well i don't wish to see it any more observed phoebe turning away her eyes it is\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.75 ---\n",
      "Chunk 37: well i don't wish to see it any more observed phoebe turning away her eyes it is certain\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.09 ---\n",
      "Chunk 38: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.14 ---\n",
      "Chunk 39: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.99 ---\n",
      "Chunk 40: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very like\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 8.03 ---\n",
      "Chunk 41: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very like\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.67 ---\n",
      "Chunk 42: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very like the old\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.82 ---\n",
      "Chunk 43: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very like the old\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.71 ---\n",
      "Chunk 44: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very like the old por\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.01 ---\n",
      "Chunk 45: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very like the old portrait\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.81 ---\n",
      "Chunk 46: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very like the old portrait\n",
      "--- 0.02 seconds ---\n",
      "--- RTF 7.60 ---\n",
      "Chunk 47: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very like the old portrait\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import time\n",
    "from asr_streaming_object import ASRStreaming\n",
    "\n",
    "# Initialize the ASRStreaming object\n",
    "asr_streaming = ASRStreaming(model_name=\"stt_en_fastconformer_hybrid_large_streaming_multi\", lookahead_size=80, decoder_type=\"rnnt\")\n",
    "\n",
    "# Load the audio file\n",
    "y, sr = librosa.load('2086-149220-0033.wav', sr=16000)\n",
    "\n",
    "# Define the chunk duration in seconds\n",
    "sr = 16000\n",
    "chunk_duration = 0.160  # seconds\n",
    "chunk_samples = int(sr * chunk_duration) - 1  # samples per chunk\n",
    "print(chunk_samples)\n",
    "\n",
    "# Calculate the number of chunks\n",
    "num_chunks = int(np.ceil(len(y) / chunk_samples))\n",
    "print(num_chunks)\n",
    "\n",
    "# Process each chunk\n",
    "for i in range(num_chunks):\n",
    "    start_sample = i * chunk_samples\n",
    "    end_sample = min((i + 1) * chunk_samples, len(y))\n",
    "    chunk = y[start_sample:end_sample]\n",
    "    \n",
    "    # If the chunk is shorter than chunk_samples, pad with zeroes\n",
    "    if len(chunk) < chunk_samples:\n",
    "        padding = chunk_samples - len(chunk)\n",
    "        chunk = np.pad(chunk, (0, padding), mode='constant')\n",
    "\n",
    "    chunk = (chunk * 32767).astype(np.int16)\n",
    "\n",
    "    start_time = time.time()\n",
    "    text = asr_streaming.transcribe_chunk(chunk)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"--- {elapsed_time:.2f} seconds ---\")\n",
    "    print(f\"--- RTF {chunk_duration / elapsed_time:.2f} ---\")\n",
    "    print(f\"Chunk {i + 1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:54:57 cloud:58] Found existing object /home/sander/.cache/torch/NeMo/NeMo_2.0.0/stt_en_fastconformer_hybrid_large_streaming_multi/a22a1091ef0b90f40b4e99859c44b15e/stt_en_fastconformer_hybrid_large_streaming_multi.nemo.\n",
      "[NeMo I 2024-12-06 15:54:57 cloud:64] Re-using file from: /home/sander/.cache/torch/NeMo/NeMo_2.0.0/stt_en_fastconformer_hybrid_large_streaming_multi/a22a1091ef0b90f40b4e99859c44b15e/stt_en_fastconformer_hybrid_large_streaming_multi.nemo\n",
      "[NeMo I 2024-12-06 15:54:57 common:826] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2024-12-06 15:54:58 mixins:173] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 15:54:59 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - - /raid/local//bucket1/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket2/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket3/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket4/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket5/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket6/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket7/tarred_audio_manifest.json\n",
      "    - - /raid/local//bucket8/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 25\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket1/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket2/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket3/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket4/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket5/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket6/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket7/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0//bucket8/audio__OP_0..8191_CL_.tar\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size:\n",
      "    - 72\n",
      "    - 64\n",
      "    - 56\n",
      "    - 48\n",
      "    - 40\n",
      "    - 32\n",
      "    - 24\n",
      "    - 16\n",
      "    \n",
      "[NeMo W 2024-12-06 15:54:59 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2024-12-06 15:54:59 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:54:59 features:305] PADDING: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 15:55:00 nemo_logging:349] /home/sander/miniconda3/envs/expand/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:55:00 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2024-12-06 15:55:00 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 15:55:00 rnnt_loop_labels_computer:270] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: No `cuda-python` module. Please do `pip install cuda-python>=12.3`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:55:00 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 15:55:00 rnnt_loop_labels_computer:270] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: No `cuda-python` module. Please do `pip install cuda-python>=12.3`\n",
      "[NeMo W 2024-12-06 15:55:00 nemo_logging:349] /home/sander/miniconda3/envs/expand/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:682: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:55:00 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/sander/.cache/torch/NeMo/NeMo_2.0.0/stt_en_fastconformer_hybrid_large_streaming_multi/a22a1091ef0b90f40b4e99859c44b15e/stt_en_fastconformer_hybrid_large_streaming_multi.nemo.\n",
      "[NeMo I 2024-12-06 15:55:00 hybrid_rnnt_ctc_bpe_models:431] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2024-12-06 15:55:00 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 15:55:00 rnnt_loop_labels_computer:270] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: No `cuda-python` module. Please do `pip install cuda-python>=12.3`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 15:55:00 hybrid_rnnt_ctc_bpe_models:469] Changed decoding strategy of the RNNT decoder to \n",
      "    model_type: rnnt\n",
      "    strategy: greedy_batch\n",
      "    compute_hypothesis_token_set: false\n",
      "    preserve_alignments: null\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    fused_batch_size: null\n",
      "    compute_timestamps: null\n",
      "    compute_langs: false\n",
      "    word_seperator: ' '\n",
      "    rnnt_timestamp_type: all\n",
      "    greedy:\n",
      "      max_symbols_per_step: 10\n",
      "      preserve_alignments: false\n",
      "      preserve_frame_confidence: false\n",
      "      tdt_include_duration_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "      loop_labels: true\n",
      "      use_cuda_graph_decoder: true\n",
      "      max_symbols: 10\n",
      "    beam:\n",
      "      beam_size: 5\n",
      "      search_type: default\n",
      "      score_norm: true\n",
      "      return_best_hypothesis: false\n",
      "      tsd_max_sym_exp_per_step: 50\n",
      "      alsd_max_target_len: 2.0\n",
      "      nsc_max_timesteps_expansion: 1\n",
      "      nsc_prefix_alpha: 1\n",
      "      maes_num_steps: 2\n",
      "      maes_prefix_alpha: 1\n",
      "      maes_expansion_gamma: 2.3\n",
      "      maes_expansion_beta: 2\n",
      "      language_model: null\n",
      "      softmax_temperature: 1.0\n",
      "      preserve_alignments: false\n",
      "      ngram_lm_model: null\n",
      "      ngram_lm_alpha: 0.0\n",
      "      hat_subtract_ilm: false\n",
      "      hat_ilm_weight: 0.0\n",
      "      tsd_max_sym_exp: 50\n",
      "    temperature: 1.0\n",
      "    durations: []\n",
      "    big_blank_durations: []\n",
      "    \n",
      "[NeMo I 2024-12-06 15:55:00 rnnt_models:225] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2024-12-06 15:55:00 rnnt_decoding:730] Joint fused batch size <= 0; Will temporarily disable fused batch step in the Joint.\n",
      "[NeMo I 2024-12-06 15:55:00 hybrid_rnnt_ctc_bpe_models:469] Changed decoding strategy of the RNNT decoder to \n",
      "    model_type: rnnt\n",
      "    strategy: greedy\n",
      "    compute_hypothesis_token_set: false\n",
      "    preserve_alignments: false\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    fused_batch_size: -1\n",
      "    compute_timestamps: null\n",
      "    compute_langs: false\n",
      "    word_seperator: ' '\n",
      "    rnnt_timestamp_type: all\n",
      "    greedy:\n",
      "      max_symbols_per_step: 10\n",
      "      preserve_alignments: false\n",
      "      preserve_frame_confidence: false\n",
      "      tdt_include_duration_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "      loop_labels: true\n",
      "      use_cuda_graph_decoder: true\n",
      "      max_symbols: 10\n",
      "    beam:\n",
      "      beam_size: 5\n",
      "      search_type: default\n",
      "      score_norm: true\n",
      "      return_best_hypothesis: false\n",
      "      tsd_max_sym_exp_per_step: 50\n",
      "      alsd_max_target_len: 2.0\n",
      "      nsc_max_timesteps_expansion: 1\n",
      "      nsc_prefix_alpha: 1\n",
      "      maes_num_steps: 2\n",
      "      maes_prefix_alpha: 1\n",
      "      maes_expansion_gamma: 2.3\n",
      "      maes_expansion_beta: 2\n",
      "      language_model: null\n",
      "      softmax_temperature: 1.0\n",
      "      preserve_alignments: false\n",
      "      ngram_lm_model: null\n",
      "      ngram_lm_alpha: 0.0\n",
      "      hat_subtract_ilm: false\n",
      "      hat_ilm_weight: 0.0\n",
      "      tsd_max_sym_exp: 50\n",
      "    temperature: 1.0\n",
      "    durations: []\n",
      "    big_blank_durations: []\n",
      "    \n",
      "[NeMo I 2024-12-06 15:55:00 features:305] PADDING: 0\n",
      "Available audio input devices:\n",
      "4 HD Pro Webcam C920: USB Audio (hw:1,0)\n",
      "5 HD-Audio Generic: ALC1220 Analog (hw:2,0)\n",
      "7 HD-Audio Generic: ALC1220 Alt Analog (hw:2,2)\n",
      "9 jack\n",
      "10 pipewire\n",
      "11 pulse\n",
      "12 default\n",
      "13 Starship/Matisse HD Audio Controller Analog Stereo\n",
      "14 HD Pro Webcam C920 Analog Stereo\n",
      "15 Firefox\n",
      "16 speech-dispatcher-dummy\n",
      "Please type input device ID:\n",
      "Listening...\n",
      "slippery sloppery dew how are you she saw cats in the neighbor's house a pink shell was found on the sandy beach\n",
      "PyAudio stopped\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mis_active():\n\u001b[0;32m---> 54\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:        \n\u001b[1;32m     56\u001b[0m     stream\u001b[38;5;241m.\u001b[39mstop_stream()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyaudio as pa\n",
    "import numpy as np\n",
    "import time\n",
    "from asr_streaming_object import ASRStreaming\n",
    "\n",
    "# INIT PARAMS\n",
    "lookahead_size = 80\n",
    "# specify encoder step length (which is 80 ms for FastConformer models)\n",
    "ENCODER_STEP_LENGTH = 80 # ms\n",
    "SAMPLE_RATE = 16000\n",
    "chunk_duration = (lookahead_size + ENCODER_STEP_LENGTH) / 1000  # seconds\n",
    "chunk_samples = int(SAMPLE_RATE * chunk_duration) - 1  # samples per chunk\n",
    "# Initialize the ASRStreaming object\n",
    "asr_streaming = ASRStreaming(model_name=\"stt_en_fastconformer_hybrid_large_streaming_multi\", lookahead_size=lookahead_size, decoder_type=\"rnnt\")\n",
    "\n",
    "# MICROPHONE EXAMPLE\n",
    "p = pa.PyAudio()\n",
    "print('Available audio input devices:')\n",
    "input_devices = []\n",
    "for i in range(p.get_device_count()):\n",
    "    dev = p.get_device_info_by_index(i)\n",
    "    if dev.get('maxInputChannels'):\n",
    "        input_devices.append(i)\n",
    "        print(i, dev.get('name'))\n",
    "\n",
    "if len(input_devices):\n",
    "    dev_idx = -2\n",
    "    while dev_idx not in input_devices:\n",
    "        print('Please type input device ID:')\n",
    "        dev_idx = int(input())\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        signal = np.frombuffer(in_data, dtype=np.int16)\n",
    "        text = asr_streaming.transcribe_chunk(signal)\n",
    "        print(text, end='\\r')\n",
    "        return (in_data, pa.paContinue)\n",
    "\n",
    "    stream = p.open(format=pa.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=SAMPLE_RATE,\n",
    "                    input=True,\n",
    "                    input_device_index=dev_idx,\n",
    "                    stream_callback=callback,\n",
    "                    frames_per_buffer=chunk_samples\n",
    "                   )\n",
    "\n",
    "    print('Listening...')\n",
    "\n",
    "    stream.start_stream()\n",
    "    \n",
    "    # Interrupt kernel and then speak for a few more words to exit the pyaudio loop !\n",
    "    try:\n",
    "        while stream.is_active():\n",
    "            time.sleep(0.1)\n",
    "    finally:        \n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        print()\n",
    "        print(\"PyAudio stopped\")\n",
    "    \n",
    "else:\n",
    "    print('ERROR: No audio input device found.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
